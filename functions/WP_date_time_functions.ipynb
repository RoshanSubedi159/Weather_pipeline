{"cells":[{"cell_type":"markdown","source":["### Creating and Loading Date table"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"952319aa-035b-407a-901d-35cf98690950","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def load_date_table(start_date = '2023-01-01', end_date = '2023-12-31'):\n    #importing libraries\n    from pyspark.sql import functions as fn\n    from pyspark.sql.types import IntegerType\n    from pyspark.pandas import date_range\n    \n    # creating the date range to load into the date table\n    date_df = spark.createDataFrame(date_range(start_date, end_date).to_numpy(),['Date'])\n    \n    # creating all the necessary attributes required for date_table\n    date_df = date_df.withColumn(\"date_key\", fn.date_format('Date', 'yyyyMMdd').cast(IntegerType()))\n    date_df = date_df.withColumn(\"full_date\", fn.date_format('Date', 'yyyy-MM-dd').cast('DATE'))\n    date_df = date_df.withColumn(\"month_name\", fn.date_format('Date', 'MMMM'))\n    date_df = date_df.withColumn(\"month_num_of_year\", fn.month('Date'))\n    date_df = date_df.withColumn(\"day_num_of_week\", fn.date_format('Date', 'EEEE'))\n    date_df = date_df.withColumn(\"day_name_of_week\", fn.dayofweek('Date'))\n    date_df = date_df.withColumn(\"day_num_of_month\", fn.dayofmonth('Date'))\n    date_df = date_df.withColumn(\"day_num_of_year\", fn.dayofyear('Date'))\n    date_df = date_df.withColumn(\"week_num_of_year\", fn.weekofyear('Date'))\n    date_df = date_df.withColumn(\"quarter_name\", fn.concat(fn.lit(\"Q\"), fn.quarter('Date')))\n    date_df = date_df.withColumn(\"calendar_quarter\", fn.quarter('Date'))\n    date_df = date_df.withColumn(\"calendar_year\", fn.year('Date'))\n    date_df = date_df.drop('Date')\n    \n    # loading the created dataframe into dim_date_table\n    date_df.write.format('delta').mode('overwrite').saveAsTable('dim_date_table')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"2ed08ffd-3906-4173-a2f0-d01560f86214","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Creating and loading time table"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"75856dda-530d-4d6b-a0c4-bbd91a8f487a","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def load_time_table():\n    # loading required libraries\n    from pyspark.sql.functions import col\n    from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n    \n    # creating the time_table schema\n    schema = StructType([\n    StructField(\"timeID\", IntegerType(), nullable=False),\n    StructField(\"startTime\", StringType(), nullable=False),\n    StructField(\"endTime\", StringType(), nullable=False)\n    ])\n    \n    # creating the data to load into the time table\n    time_ranges = []\n    for hour in range(24):\n        start_time = f\"{hour:02d}:00\"\n        end_time = f\"{hour:02d}:59\"\n        time_ranges.append((hour, start_time, end_time))\n        \n    # loading the data into dim_time_table\n    time_df = spark.createDataFrame(time_ranges, schema)\n    time_df.write.format('DELTA').mode('overwrite').saveAsTable('dim_time_table')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"175e17af-45b9-41e2-a24d-008c845c5377","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"WP_date_time_functions","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
