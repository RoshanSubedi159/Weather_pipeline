{"cells":[{"cell_type":"markdown","source":["### Defining schema for Log Table"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f1a6e8ac-ac7b-448c-abec-df442ef2aa3b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def create_log_table():\n    spark.sql(\"\"\"CREATE TABLE IF NOT EXISTS log_table(\\\n      id STRING,\\\n      load_type STRING,\\\n      table_name STRING,\\\n      process_start_time TIMESTAMP,\\\n      process_end_time TIMESTAMP,\\\n      status STRING,\\\n      comments STRING,\\\n      start_date_time TIMESTAMP,\\\n      end_date_time TIMESTAMP,\\\n      created_on TIMESTAMP,\\\n      created_by STRING\\\n    )\\\n    USING DELTA;\"\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000,"implicitDf":true},"nuid":"29a5ce8c-e879-4652-a1a3-fa32a2314670","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Defining the decorator for logging"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"68b82624-27f8-4919-b603-910b52156f6d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def logger(func):\n    def wrapper(*args, **kwargs):\n      \n        import uuid\n        from datetime import datetime\n        from pyspark.sql.functions import col, udf\n        from pyspark.sql.types import TimestampType\n        \n        id = str(uuid.uuid4())\n        load_type = args[0]\n        table_name = args[1]\n        process_start_dt = datetime.now()\n        name = 'Roshan Subedi'\n        status = 'EXTRACTING'\n \n        query = f\"insert into log_table (id, load_type, table_name, process_start_time, status, created_on, created_by)\\\n            values ('{id}', '{load_type}', '{table_name}', '{process_start_dt}', '{status}', '{process_start_dt}', '{name}')\"\n        spark.sql(query)\n        \n        try:\n            df, start, end = func(*args[2:], **kwargs)\n            status = 'COMPLETED'\n        except Exception as e:\n            status = 'ERROR'\n            process_end_dt = datetime.now()\n            query = f\"update log_table set process_end_time = '{process_end_dt}', status='{status}', comments='{e}' where id='{id}'\"\n            spark.sql(query)\n            dbutils.notebook.exit(1)\n            \n            \n        udf_id = udf(lambda : id)\n        udf_created_on = udf(lambda : process_start_dt, TimestampType())\n        udf_created_by = udf(lambda : name)\n        \n        df = df.withColumn('load_run_id', udf_id())\n        df = df.withColumn('created_on', udf_created_on())\n        df = df.withColumn('created_by', udf_created_by())\n        \n        df.write.format('delta').mode('append').saveAsTable(table_name)\n        \n        process_end_dt = datetime.now()\n        query = f\"update log_table \\\n                    set process_end_time = '{process_end_dt}', status='{status}', start_date_time='{start}', end_date_time='{end}'\\\n                    where id='{id}'\"\n        spark.sql(query)\n        \n        return df\n    \n    return wrapper"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f9d99323-f0f9-4445-bf1b-9de9214c4711","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"WP_log_functions","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":3097828204238210,"dataframes":["_sqldf"]}},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
